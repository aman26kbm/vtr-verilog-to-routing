List of benchmarks and paths to related collateral

1. TPU v1
https://github.com/aman26kbm/tpu_like_design/tree/master/for_vtr_bench_suite
Several enhancements are planned in the future
 * Update the pooling design
 * Change to input/weight stationary matmul
 * Use output chaining to connect DSPs
 * Use the matmul (already developed in the mycores repo) that support back-to-back computation
 * Fix rouding in the MAC. Need to accumulate in higher precision

2. GEMM layer
The matmul design is here:
https://github.com/aman26kbm/mycores/tree/main/matmul

The AXI interface is taken from here:
https://github.com/aman26kbm/tpu_like_design/tree/master/vivado_files
Go to folder matmul_8x8_with_cdma.xpr\matmul_8x8_with_cdma\matmul_8x8_with_cdma.ipdefs\matmul_1.0_0\hdl

3. Robot RL
All the collateral is here:
https://github.com/samidh99/Internship_2021/tree/main/robot_files

4. Attention
https://github.com/Aishwarya-8/LSTM-and-Attention-Layer-Designs

The softmax block used inside it came from https://github.com/georgewzg95/softmax/blob/master/softmax_modified/softmax_p4_fixed16_mem_lut.v
It's also checked in here: https://github.com/georgewzg95/softmax/blob/master/generator/results/softmax_fixed16.mar15.v

5. LSTM
https://github.com/Aishwarya-8/LSTM-and-Attention-Layer-Designs

6. Tiny Darknet
https://github.com/UT-LCA/scale-cnn/tree/main/vtr

7. SpMV
https://github.com/aatmanb/SpMV/tree/main/src

8. Softmax
The main repo that contains the softmax design is here:
https://github.com/georgewzg95/softmax

The design we used was softmax_p8_smem_rfloat16_alut_v512_b2_-0.1_0.1.v
But we made some modifications to the design generated by the generator. Read this file
https://github.com/georgewzg95/softmax/tree/master/softmax_modified

The final version that was checked in is here: 
https://github.com/georgewzg95/softmax/blob/master/softmax_modified/softmax_p8_fp16_mem_lut.v

9. Conv layer (non-hls)
https://github.com/aman26kbm/mycores/tree/main/conv

10. Conv layer (hls)
This was a byproduct of the tiny darknet hls design. See the link above.

11. Reduction
https://github.com/aman26kbm/mycores/tree/main/reduction

12. Eltwise
https://github.com/aman26kbm/mycores/tree/main/eltwise

13. BNN
Used example models from hls4ml. See this: https://github.com/fastmachinelearning/example-models/tree/d4222da78f24b488ecc01dbaaa481fb43ccdc5e2/keras
The master branch is here: https://github.com/fastmachinelearning/example-models/tree/master/keras

The hls4ml tool repo is here: https://github.com/fastmachinelearning/hls4ml

I've checked in the files I used here:
https://github.com/aman26kbm/mycores/tree/main/hls4ml

14. CLSTM
Original repo with Zach's design in SystemVerilog
https://github.com/zachzzc/DL_FPGA_benchmark_suite/tree/master/C_LSTM

15. DLA
Original repo with Andrew's design in SystemVerilog
https://github.com/andrewboutros/FPGA-DL-benchmarks

---------------------------------------------------------
Usage of local FP modules when complex_dsp is not defined
---------------------------------------------------------
attention_layer.v -> Changed to use pipelined addsub
conv_layer_hls.v -> Not changing to use pipelined FP units. Original HLS generated code doesn't generated pipelined fp blocks.
eltwise_layer.v -> Changed to use pipelined addsub and mult
gemm_layer.v -> Changed to use pipelined addsub and mult
softmax.v -> Changed to use pipelined addsub and mult 
tiny_darknet_like.*.v -> Not changing to use pipelined FP units. Original HLS generated code doesn't generated pipelined fp blocks.
